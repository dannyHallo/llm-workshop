# 拟人化送祝福数据的制造流程

推荐准备：

- Claude 3.5 sonnet
- Qwen2.5 72B 或 deepseek v3

在本文中，我们将展示如何制造更加拟人化的送祝福数据，进而能够产出高质量的拟人化送祝福大模型。

微调高度拟人的模型的第一步是准备拟人化的训练数据。对于一个送祝福模型，你需要收集各种祝福语的数据，数据来源可以是公开的祝福语数据集、社交媒体、电子书籍或者任何包含丰富祝福语的文本。

然而，利用大语言模型生成拟人化的文本并不是一件容易的事，我们会发现绝大部分的模型生成的结果或者改写后的结果都会有很严重的“助手化”倾向，或看上去句式非常官方。

为解决该问题，经过几十款大模型的生成效果评测，最后发现，只有利用 **Claude 3.5 sonnet** 才能够生成极高拟人化的祝福语，然后再利用 **Qwen2.5 72B** 大模型进行内容改写，就能够实现高质量拟人祝福语的批量制造。 

为什么需要如此分开进行操作，而不是仅使用 gpt 或 qwen 的模型直接批量制作数据？其原因是当前几乎没有任何一款大模型可以实现 “学会某个风格语气，但句式完全不相同”，几乎所有的大模型都可以实现 “保持句式不变的情况下实现修改用词、句子的主题” 但仍然拥有极大概率与原数据查重率90%以上，其中句法结构几乎观察不到改变。如果你尝试让他修改句式条件，则大概率会让句子无法遵守原风格，逐渐 “助手化”，产生问答助手风格的文字，无法实行拟人化模型的需求。

以该句子为例：

```
天光乍暖，窗外的玉兰花悄悄绽放了。晨起时发现枝头多了几朵白色的小精灵，花瓣边缘还带着露水。阳光透过树叶的缝隙，在地上织出斑驳的光影，仿佛时光都在这一刻静止。书桌上散落着几张速写，都是随手画下的花草。最近迷上了用铅笔勾勒生活的细节，把每个微小的感动都留在纸上。茶杯里的红茶已经凉了，却依然散发着淡淡的茉莉香。有时候会想，这样的时光真好，慢得刚刚好。日子总是这样，不紧不慢地向前，带着我们走过四季更迭。
```

若使用非 claude 任意模型进行修改，得到的结果(无法重构句式、语句风格仍然AI助手明显）：

```
在淡淡的茉莉香气中，我发现茶杯里的红茶已渐渐冷却，但那香气却依然萦绕不散。书桌上散落的几张速写，记录着我用铅笔捕捉的那些生活中的细微感动。最近，我沉迷于描绘这些花草，每一笔都承载着我对生活的热爱。窗外，玉兰花悄然绽放，晨光中，几朵白色的小花在枝头轻轻摇曳，花瓣上还挂着晶莹的露珠。阳光透过树叶的缝隙，地面上斑驳的光影仿佛定格了时光。这样的时刻，慢得刚刚好，让人感到无比的宁静与美好。生活以一种不疾不徐的步伐，不仅带领我们体验每一个季节的变换，更教会我们珍惜每一个平凡而又珍贵的瞬间。在这样的时光里，我们学会了放慢脚步，用心感受世界的美好，从而在忙碌与喧嚣中找到内心的宁静和力量。
```

Claude 3.5 可以实现，在保持绝对风格与元数据相似的情况下，改写句子内容；换句话说，他能够做到句子结构完全不同，但仍然拥有相似的说话风格：

```
午后的咖啡馆角落，阳光透过玻璃窗洒进来，在木质桌面上留下斑驳的光影。桌上的拿铁还冒着热气，奶泡上的拉花慢慢散开，像朵绽放的花。对面的你正低头看书，时不时推一下镜框，这个画面让我想起很多年前的大学时光。那时我们也是这样，一个看书，一个发呆，阳光暖暖的，时光缓缓的。现在想来，那些平凡的日子竟是最珍贵的回忆。
店里放着轻柔的爵士乐，混着咖啡机运作的声音，还有街道上偶尔传来的说笑声。这座城市总是这样，热闹中带着宁静。就像我们的友情，不需要太多言语，只要能这样静静地坐着，就已经很美好。外面的天空蓝得让人心醉，飘过的白云像棉花糖一样蓬松。有时候觉得，生活就该是这个样子，不紧不慢，带着温度，还有一些说不清道不明的感动。
```

接下来演示具体的数据制造过程：

### 一、元数据获取

此处可以从各类书本、网页进行摘抄，选取某个风格语句的一部分段落，即可作为初始数据制造的元数据，此处以文艺风格为例，选取自己满意的几个段落：

> 五月的黎明，是最温柔的。隔着纱帘都能闻见木槿的清香，伴着叮当的风铃声，仿佛整个世界都在等待一场美好的相遇。
恰好你就是这样的存在。不温不火却总能让人感到暖意，会在别人难过时轻轻拍拍肩，也会为朋友的快乐开怀大笑。你像一束落在平凡日子里的光，温和又特别。你的笑容啊，总让我想起院子里刚绽放的丁香，明亮又真挚。
>
> 小区的楼影间，天光渐暗，几只麻雀叽叽喳喳地跳跃着。阳台上摆着的绿植随风轻摆，一片叶子正好挡住远处的霓虹。这个城市总是这样，安静中带着喧嚣哦，就像我此刻的心情。桌上的咖啡还冒着热气，杯壁上有细密的水珠在凝结。最近总是喜欢在这个时候坐着发呆，看着窗外的天色渐渐暗下来。楼下小孩子的笑声偶尔传来，让我想起很多年前的自己。那时候也是这样，天还没黑就跑出去玩，妈妈在楼上喊着让回家吃饭。生活就是这样啊，时光静静流淌，走了又留下，来去匆匆。

**收集几种语料（每种里面3~5段）即为元数据储备结束**，接下里需要对元数据进行风格指定、句式不同的改写，获得可用于 SFT 阶段的微调数据。

### 二、初始数据获取

在这一步，我们将学习如何制造拟人数据的“元数据”，也就是 Claude 生成的最初祝福语。

我们使用下列提示词（任选一种），结合上述元数据的例子，即可进行数据制造。

```
祝愿朋友生日快乐，严格参考类似的语气，随机参考一段返回给我新的一个段落就好（可以假设和对方之间发生的故事和关系）：

参考这个写一段话，严格参考风格，段落不用严格参考 我指的是字数、分段不用按照这个话 需要完全修改句式段落，你要参考这个的修辞手法、说话的语气：

需要完全修改句式段落，开头和结尾，以及过程中的句子组成语法重构

开头和结尾的变化需要更大
```

例如，我们可以这样和 Claude 3.5 进行对话：

```
参考这个写一段话，严格参考风格，段落不用严格参考 我指的是字数、分段不用按照这个话 需要完全修改句式段落，你要参考这个的修辞手法、说话的语气：<此处粘贴元数据>
```

此时，Claude 3.5 会返回新的段落（如果风格有偏移，可以和它再次**强调，一定要遵守风格**），可能这时候只是高度遵守了结构，换了主题，但仍然还看得出和元数据的句式高度相似，此时我们只需要使用类似的提示词：

```
你需要完全修改句式段落，开头和结尾，以及过程中的句子组成语法重构

开头和结尾的变化需要更大
```

此时即可获得句式改变更大，但风格一致的祝福语（如果偏移，你可以和他强调一定要遵守风格），最后结果如下：

```
装进信封的不是信件，是夏夜萤火虫的微光，是清晨玻璃窗上的水雾，是楼下老榕树的沙沙声，是我写下这些文字时指尖的温度，是藏在口袋里的一枚幸运石。
总在深夜里想逃离，逃到没有闹钟的小岛，做一只栖息在树梢的知更鸟，不用回复任何消息，只在晨曦里啼唱，在暮色中安眠。
有时会在地铁里偷偷落泪，为身边陌生人眼里的故事，为街角老店的招牌忽然不见，为时光太匆匆，甚至来不及和昨天的自己道别。情绪就像手心里的沙，握得越紧，流失得越快。
生活的美好总在不经意间，转角遇见的流浪猫伸了个懒腰，书店里翻到一张褪色的书签，咖啡杯上升起的雾气画出的图案，邻居阿姨种的蝴蝶兰新开的花。这些片段像风铃，轻轻碰撞就会发出清脆的声响。
```

此时，**需要你保持该对话状态（状态为多轮对话）**，让他按照相同的风格和不同的主题持续书写对应的内容即可。

我们可以参考该方式，持续造出几十种基础数据（可以人工操作，因为数量不多），之后再利用任意大模型（此时不需要 claude 也可以做的很好）进行微调即可解决。

### 三、微调数据获取

我们可以很容易获得微调数据，只需要指定主题进行修改，以下列段落为例：

> 有时会在地铁里偷偷落泪，为身边陌生人眼里的故事，为街角老店的招牌忽然不见，为时光太匆匆，甚至来不及和昨天的自己道别。情绪就像手心里的沙，握得越紧，流失得越快。

我们可以很容易利用任意大模型进行主题修改，例如：

> 偶尔会在春日里悄然感怀，为枝头新芽的悄然绽放，为溪边柳絮的轻盈飘散，为季节更迭，转眼间冬日的沉寂已被生机取代。心绪恰似春风拂面，愈是追寻，愈是难以捉摸。

> 常常在黄昏时分黯然神伤，为夕阳下老槐树的影子拉长，为街角小贩的叫卖声渐渐远去，为岁月流转，转眼间稚嫩的童年已成过往。

> 常常在秋日里独自徘徊，为落叶的飘零，为秋风的萧瑟，为岁月流转，转眼间稚嫩的童年已成过往。

> 常常在冬日里默默祈祷，为雪花的飘落，为寒风的刺骨，为岁月流转，转眼间稚嫩的童年已成过往。

> 常常在春日里悄然感怀，为枝头新芽的悄然绽放，为溪边柳絮的轻盈飘散，为季节更迭，转眼间冬日的沉寂已被生机取代。心绪恰似春风拂面，愈是追寻，愈是难以捉摸。

此外，也可参考相同语句风格与修辞，制造出新的祝福语句：

```
请你严格参考这个语句风格写一段祝福小明生日快乐的祝福语： 常常在春日里悄然感怀，为枝头新芽的悄然绽放，为溪边柳絮的轻盈飘散，为季节更迭，转眼间冬日的沉寂已被生机取代。心绪恰似春风拂面，愈是追寻，愈是难以捉摸。

大模型回复如下：

在这特别的日子里，愿你的生命如春日般绚烂多彩，如新芽般充满希望与活力。岁月流转，愿你始终如溪边柳絮，轻盈自在，随风起舞。生日之际，愿你的每一个梦想都如春风般温柔拂过心田，带来无尽的喜悦与美好。小明，生日快乐！
```

但对于绝大多数大模型，可能无法真正拟人化实现针对主题修改为祝福语，我们同样可以利用 claude 先制造出参考数据，例如：

> 常常在这明媚时光里静静思念，为你眼中璀璨的星光闪耀，为你唇边温柔的笑意绽放，为岁月流转，转眼间昨日的稚嫩已被成熟沉淀。心意如同晨露轻抚，愈是珍藏，愈显温润动人。愿你踏着春日的暖阳，携着满怀的憧憬，在人生的画卷上描绘出属于你的绚丽篇章。快乐永驻，小明。

此时可随意让任意大模型进行主题修改，例如：

> 常常在这辞旧迎新的时刻深深祝福，为你眼中那份不变的坚定与希望，为你嘴角那抹熟悉的温暖与从容，为时光荏苒，昔日的青涩已化作今日的沉稳。心意如同新年的初雪，轻轻落下，纯净而美好。愿你在新年的晨曦中，迎着和煦的朝阳，怀揣着无尽的梦想，在生命的旅途中书写出更加辉煌的篇章。新年快乐，姐姐。

> 在这粽叶飘香的时节，心中泛起阵阵温馨的涟漪，为你那如龙舟般勇往直前的精神，为你那似艾草般清新脱俗的气质，为岁月悠悠，同窗时光已成珍贵回忆。心意如同端午的细雨，润物无声，却饱含深情。愿你在端午的阳光下，乘着和风，驾驭着梦想的舟楫，在人生的江河中破浪前行。端午安康，我亲爱的同学。

此处可参考送祝福[最新版数据制造脚本](../../tools/finetune/data_maker/get_wish_datav2.py)，进行大批量数据的生成，即可获得大量微调数据。至此，拟人化数据准备完成。

### 数据合并

因为我们之前的数据都是跑完一轮存一次（以防前功尽弃），所以可能你有多个json需要组合，这里提供了一个脚本合并一个文件夹中的所有json，并把json格式清洗成和训练脚本一致适配的格式：

```bash
import os
import json

def extract_and_merge_conversations(folder_path, output_file):
    all_conversations = []

    # 遍历指定文件夹
    for filename in os.listdir(folder_path):
        if filename.endswith('.json'):
            file_path = os.path.join(folder_path, filename)

            # 打开并读取JSON文件
            with open(file_path, 'r', encoding='utf-8') as file:
                data = json.load(file)
                # 提取需要的字段
                for item in data:
                    for conversation in item['conversation']:
                        extracted = {
                            'system': conversation['system'],
                            'input': conversation['input'],
                            'output': conversation['output']
                        }
                        # 将每个对话包装在一个 'conversation' 键中，并作为独立对象加入列表
                        all_conversations.append({'conversation': [extracted]})

    # 将合并后的所有对话数据写入一个新的JSON文件
    with open(output_file, 'w', encoding='utf-8') as file:
        json.dump(all_conversations, file, ensure_ascii=False, indent=4)

# 使用示例
folder_path = 'tianji_wishes_datasets'  # 要扫描的文件夹路径
output_file = 'tianji-wishes-chinese-v0.1.json'     # 输出文件的名称和路径
extract_and_merge_conversations(folder_path, output_file)
```

合并后就是我们所需要的微调数据集。

### 二次清洗

得到最初数据后，可能还存在一些奇怪的东西，比如句子长度返回错误，不是回答而是很短的一句 `当前祝福语如下` ，加入语气助词后容易出现 `！啦~` `。哦！` 如此在前面出现标点的奇怪现象，故我们需要利用清洗脚本对数据进行一定的筛选，由于比较冗长（还没有优雅的迭代），清洗脚本放在 [https://github.com/SocialAI-tianji/Tianji](https://github.com/SocialAI-tianji/Tianji)，可以自行查看

当然，如果你有时间和精力，最重要的还是可以用人工进行清洗。当然，**最重要的还是最好在数据制造的时候就记录比较完好的信息和做出比较好的output，能大大节约二次清洗的时间。**

### 总结

至此，我们已经完成了数据集的制造。接下来就可以使用 transformers 等微调框架进行模型的训练，祝愿你能够训练出自己满意的拟人化真实大模型。
